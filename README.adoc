= HR Technical challenge for data engineering skills assessment
:hardbreaks-option:
Hiring Manager: Kirsten Edwards <kirsten.edwards@riotinto.com>
v1.1, May 2022
:url-repo: https://github.com/Joe-Matthew-RIO/de-challenge-2022

NOTE: The purpose of this assessment is to understand skills and experience with:

- Big data processing
- Data wrangling
- Cloud Engineering / Orchestration
- Data pipeline competence 

There are two ways options below to demonstrate these: 


# Option 1 (2-3 hours suggested time)

=== This challenge includes a dataset that sets the foundation for the test.
Dataset itself is named "Fraud.csv" and stored in git-lfs.
link:https://github.com/Joe-Matthew-RIO/de-challenge-2022/blob/master/Fraud.csv [Fraud Dataset]


==== About Dataset
. Dataset properties
* 6362620 records
* csv format
* File size ~ 500MB
. Field descriptions
* *step* - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).
* *type* - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.
* *amount* - amount of the transaction in local currency.
* *nameOrig* - customer who started the transaction
* *oldbalanceOrg* - initial balance before the transaction
* *newbalanceOrig* - new balance after the transaction
* *nameDest* - customer who is the recipient of the transaction
* *oldbalanceDest* - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).
* *newbalanceDest* - new balance recipient after the transaction. Note that there is no information for customers that start with M (Merchants).
* *isFraud* - These are the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control of customers' accounts and try to empty the funds by transferring to another account and then cashing out of the system.
* *isFlaggedFraud* - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200,000 in a single transaction.

=== Finally, The assignment

. Build a Data Processing pipeline takes supplied dataset from point "A", preprocesses and puts it into point "B". It's up to you what point A and point B is. Also, it's up to you if you ETL or ELT. As part of preprocessing the data:
* Count how many fraudulent transactions this dataset contains
* Count how many unique customers this dataset contains
* Create 1-day aggregates for each customer (include step, oldbalanceOrg, newbalanceOrig)
NOTE: Provide your findings and the steps taken to arrive at these results
* A data scientist has built an hourly inference model, and this model requires re-training monthly. The model is a deep-learning model deployed as an endpoint in the cloud that requires merging additional data sources. This data pipeline has a series of transformations and data integrations for the real-time endpoint (hourly) and the monthly build. 
* Draw an architecture pipeline for this inference and re-training.
* Describe some data validations you would undertake before the hourly inference and the monthly re-training jobs. 
* Describe the repeatable build process you would undertake for this architecture. (Docker compose/Terraform/Cloudformation or any other tool of your choice)
* A word document or PDF showing relevant parts of code/output for the first part and a one-page architecture depiction. 


# Option 2  

Provide a GitHub repo with some of your sample code showcasing your skills at par with Option 1. Please provide an architectural example of such a design, showing:

- Cloud deployment/orchestration for a data pipeline 
- ETL/ELT work associated with a deployment you were involved in
- Relevant software development, approach and experience
